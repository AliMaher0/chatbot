{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnM_j-P7NBA-",
        "outputId": "d1b5f1ce-697c-4231-95e5-c1fda365b092"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tflearn in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.5.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tflearn) (1.22.3)\n",
            "Requirement already satisfied: six in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tflearn) (1.16.0)\n",
            "Requirement already satisfied: Pillow in c:\\users\\lenovo\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from tflearn) (9.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "of_VSTtZiaZk"
      },
      "outputs": [],
      "source": [
        "#he Natural Language Toolkit (NLTK) is a Python package for natural language processing\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "D8udX0y08NcX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tflearn\n",
        "import random\n",
        "import json\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcjSU3cpNd9i",
        "outputId": "4167b17f-ff89-431f-ea91-9865c7b8019f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  nltk.download('punkt') #package\n",
        "except:\n",
        "  pass\n",
        "\n",
        "with open('intents.json')as file:\n",
        "  data = json.load(file)\n",
        "try:\n",
        "  with open(\"data.pickle\",\"rb\")as f:\n",
        "    words ,labels ,training,output = pickle.load(f)\n",
        "except:  \n",
        "  words = []\n",
        "  labels = []\n",
        "  docs_x = []\n",
        "  docs_y = []\n",
        "\n",
        "  for intent in data['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "      words2 = nltk.word_tokenize(pattern)\n",
        "      words.extend(words2)\n",
        "      docs_x.append(words2)\n",
        "      docs_y.append(intent['tag'])\n",
        "\n",
        "    if intent['tag'] not in labels:\n",
        "      labels.append(intent['tag'])\n",
        "  #converting each word into lowercase & removing\n",
        "  words = [stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
        "\n",
        "  #sorting and removing duplicates\n",
        "  words = sorted(list(set(words))) \n",
        "\n",
        "  labels = sorted(labels)    \n",
        "  # print(words2)\n",
        "  # print(docs_y)\n",
        "#----------------------------------------------------------------#\n",
        "  # print(docs_x)\n",
        "  training = []\n",
        "  output = []\n",
        "\n",
        "  out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "  for x,doc in enumerate(docs_x):\n",
        "    bag = []\n",
        "    words3 = [stemmer.stem(w) for w in doc]\n",
        "    for w in words:\n",
        "      if w in words3:\n",
        "        bag.append(1)\n",
        "      else:\n",
        "        bag.append(0)\n",
        "    outputRow = out_empty[:]\n",
        "    outputRow [labels.index(docs_y[x])]=1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(outputRow)\n",
        "\n",
        "  training = np.array(training)\n",
        "  output = np.array(output)\n",
        "  with open(\"data.pickle\",\"wb\")as f:\n",
        "      pickle.dump((words ,labels ,training,output),f) \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "w2uXRO8JSY2n"
      },
      "source": [
        "as the neural networks only accepts number and we have a list of words so we need to use something like one-hot encoding, this means that if the word exists we will append one (even if it exists for more than one time)\n",
        "and if it is not exist we will append 0."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wYenfE2Z9n6w"
      },
      "source": [
        "###Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BorBPH_8ijr2",
        "outputId": "6deda7ff-ddf5-4306-b40f-d9ae66496485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Step: 2999  | total loss: \u001b[1m\u001b[32m0.09218\u001b[0m\u001b[0m | time: 0.005s\n",
            "| Adam | epoch: 1500 | loss: 0.09218 - acc: 0.9966 -- iter: 16/25\n",
            "Training Step: 3000  | total loss: \u001b[1m\u001b[32m0.08837\u001b[0m\u001b[0m | time: 0.010s\n",
            "| Adam | epoch: 1500 | loss: 0.08837 - acc: 0.9969 -- iter: 25/25\n",
            "--\n",
            "INFO:tensorflow:c:\\Users\\Lenovo\\Desktop\\Fourth-level\\2ndSemester\\chatbot\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
          ]
        }
      ],
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "net  = tflearn.input_data(shape=[None,len(training[0])])#input layer\n",
        "net = tflearn.fully_connected(net,8) #hidden layer\n",
        "net = tflearn.fully_connected(net,8)#hidden layer\n",
        "net = tflearn.fully_connected(net,8)#hidden layer\n",
        "net = tflearn.fully_connected(net,8)#hidden layer\n",
        "net = tflearn.fully_connected(net,len(output[0]),activation='softmax')#output layer\n",
        "net = tflearn.regression(net)\n",
        "model = tflearn.DNN(net)\n",
        "try:\n",
        "  #model.load(\"model.tflearn\")\n",
        "  model.py\n",
        "except:\n",
        "  model.fit(training,output,n_epoch=1500, batch_size = 16 ,show_metric = True)\n",
        "  model.save(\"model.tflearn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "uMrabyqwdZZm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start talking with the bot!\n",
            "Hi there, what can I do for you?\n",
            "you can go to the gym!\n",
            "Have a nice day\n"
          ]
        }
      ],
      "source": [
        "def bag_of_words(y,words):\n",
        "  bag=[0 for _ in range(len(words))]\n",
        "  seen_words=nltk.word_tokenize(y)\n",
        "  seen_words = [stemmer.stem(word.lower())for word in seen_words]\n",
        "  for seen in seen_words :\n",
        "    for i,w in enumerate(words):\n",
        "      if w == seen :\n",
        "        bag[i] = 1\n",
        "  return np.array(bag)      \n",
        "\n",
        "def chat():\n",
        "    print(\"start talking with the bot!\")\n",
        "    while True:\n",
        "        inputs = input(\"you :\")\n",
        "        if inputs.lower() == \"quit\":\n",
        "            break\n",
        "        results = model.predict([bag_of_words(inputs, words)])\n",
        "        results_index = np.argmax(results)\n",
        "        #print(results_index)\n",
        "        tag = labels[results_index]\n",
        "      \n",
        "        for t in data[\"intents\"]:\n",
        "            if t['tag'] == tag:\n",
        "                responses = t['responses']\n",
        "        print(random.choice(responses))\n",
        "    \n",
        "chat()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
